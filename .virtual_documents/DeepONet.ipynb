
# import sys, torch, numpy as np
# print("Python:", sys.version)
# print("Torch:", torch.__version__)
# print("NumPy:", np.__version__)
# try:
#     torch.tensor([1.0]).numpy()   # will trigger the same bridge
#     print("tensor.numpy() works ✓")
# except Exception as e:
#     print("tensor.numpy() failed →", e)



# %pip install --upgrade "numpy<2"



import torch.nn as nn
import torch
import numpy as np

from matplotlib import pyplot as plt
from torch.optim import Adam



device = (
    "cuda" if torch.cuda.is_available()
    else "mps" if hasattr(torch.backends, "mps") and torch.backends.mps.is_available()
    else "cpu"
)
print(f"device = {device}")








class MLP(nn.Module):
    def __init__(self,in_features : int,out_features: int,hidden_features: int,num_hidden_layers: int) -> None:
        super().__init__()

        
        self.linear_in = nn.Linear(in_features,hidden_features)
        self.linear_out = nn.Linear(hidden_features,out_features)
        
        self.activation = torch.tanh
        self.layers = nn.ModuleList([self.linear_in] + [nn.Linear(hidden_features, hidden_features) for _ in range(num_hidden_layers)  ])
        
         
    def forward(self,x):
        for layer in self.layers:
            x = self.activation(layer(x))
    
        return self.linear_out(x)


class DeepONet(nn.Module):
    def __init__(self,latent_features,out_features,branch,trunk) -> None:
        super().__init__()
        self.branch = branch
        self.trunk = trunk
        self.fc = nn.Linear(latent_features,out_features,bias = False)
        

    def forward(self,y,u):
        return self.fc(self.trunk(y)*self.branch(u))


Onet = DeepONet(75,1,branch = MLP(100,75,75,4),trunk = MLP(1,75,75,4))

# Input to branch net - replace with chebyshev points
u = torch.rand((2,1,100))

# Input to trunk net - finer chebyshev points ?
y = torch.rand((2,40,1))

xx =Onet.trunk(y)
tt = Onet.branch(u)

Onet(y,u).shape






# from numpy.polynomial.chebyshev import chebfit, chebval

# # Example: interpolate f(x) = exp(x) on [-1,1]
# import numpy as np

# f = lambda x: np.exp(x)
# degree = 5
# x_nodes = np.cos((np.arange(degree+1) + 0.5) * np.pi / (degree+1))  # Chebyshev nodes
# y_nodes = f(x_nodes)

# coeff = chebfit(x_nodes, y_nodes, degree)  # actual Chebyshev coefficients
# x = np.linspace(-1, 1, 200)
# y = chebval(x, coeff)



from numpy.polynomial.chebyshev import Chebyshev,chebval,chebint,cheb2poly
def random_chebyshev(n,M,degree,return_coeff = False):
    '''
    This generates a random function output between the domain [-1,1] along a uniform grid of size n using chebyshev polynomials.
    '''
    coeff = (np.random.rand(degree+1)-0.5)*2*np.abs(M)
    x= np.linspace(-1,1,n)
    y = chebval(x,coeff)
    if return_coeff:
        return x,y,coeff
    else:
        return x,y




from scipy.integrate import cumulative_trapezoid
x,y,coeff = random_chebyshev(n=100,M=10,degree=30,return_coeff= True)

y_int = cumulative_trapezoid(y,x,initial = 0)

plt.plot(x,y_int)




# Generate DataTuples:
def generate_data(n_samples,a,b,n_sensors,n_points,M,degree,y0 = 0,random_query = False,seed = 1234):
    '''
    Generate Data needed for training.

    Returns a tuple of (y,u,Guy) that will be added to the Dataloader
    '''
    if seed is not None:
        np.random.seed(1234)
    
    x = np.linspace(-1,1,n_points)
    u,dy_coeff = zip(*[random_chebyshev(n_sensors,M,degree,return_coeff=True)[1:] for _ in range(n_samples)])

    u = np.stack(u,axis = 0)

    #Generate 100 y points for -1 and 1
    
    xp = np.linspace(a,b,n_sensors)
    if random_query:
        y = np.random.rand(n_samples,n_points)*(b-a)+a
    else:
        # y = np.array([np.linspace(a,b,n_points) for _ in range(n_samples)]
        y = np.tile(np.linspace(a,b,n_points),(n_samples,1))

    #Integrate cheb poly, we use interp in case we want random y points
    Guy = [cumulative_trapezoid(dy,xp,initial=y0) for dy in u]
    Guy = [np.interp(yy,xp,G) for yy,G in zip(y,Guy)]

    #Guy data needs 2 batch dimensions: One for samples and the other for  
    y,u,Guy = torch.tensor(y,dtype = torch.float32),torch.tensor(u,dtype = torch.float32),torch.tensor(Guy,dtype = torch.float32)
    #To match network shape input and output, need to add a tensor dimension to y and Guy
    return y.unsqueeze(-1),u.unsqueeze(1),Guy.unsqueeze(-1)


ys,us,Guys = generate_data(100,0,2,100,100,5,20)
ys.shape,us.shape,Guys.shape


x = np.linspace(0,2,100)

plt.scatter(ys[2].squeeze(),Guys[2].squeeze())





from torch.utils.data import DataLoader,Dataset


class Onet_dataset(Dataset):
    def __init__(self,y,u,Guy):
        self.y = y
        self.u = u
        self.Guy = Guy
        
    def __len__(self):
        return len(self.y)

    def __getitem__(self,idx):
        return self.y[idx],self.u[idx],self.Guy[idx]


ys,us,Guys = generate_data(1000,0,2,100,90,5,20)
Onet_data = Onet_dataset(ys,us,Guys)
# Onet_data.set_device('cuda')
Onet_DL = DataLoader(Onet_data,batch_size = 1000,shuffle = True)

Onet = DeepONet(75,1,branch = MLP(100,75,75,4),trunk = MLP(1,75,75,4))


optimiser = Adam(Onet.parameters(),lr = 5e-4)
lr_sch = torch.optim.lr_scheduler.StepLR(optimiser,1000,0.95)



Onet = Onet.to(device)
optimiser.zero_grad()
x = torch.linspace(0,2,100)
a = 4
dy = torch.sin(a*x)

dy_in = dy.unsqueeze(0).unsqueeze(1)
x_in = x.unsqueeze(0).unsqueeze(-1)
out_true = -1/a*(torch.cos(a*x)-1)
print(dy_in.shape,x_in.shape)

loss_list = []
acc_list = []
for epoch in range(0,5):
    for (y,u,Guy) in(Onet_DL):
        (y,u,Guy) = (y.to(device),u.to(device),Guy.to(device))
        # print(y.shape,u.shape,Guy.shape)
        out = Onet(y,u)
        # print(out.shape,Guy.shape)
        loss = (out-Guy).pow(2).mean()
        loss.backward()
        optimiser.step()
        optimiser.zero_grad()
        lr_sch.step()
    if (epoch % 100) == 0:
        print(f'{float(lr_sch.get_last_lr()[0]):.3E}')
        with torch.no_grad():
            out = Onet.cpu()(x_in,dy_in).squeeze()
            accuracy = (out-out_true).pow(2).mean()
            Onet = Onet.to(device)
        
        print(f'Epoch: {epoch} loss {float(loss):.3e} Acc {float(accuracy):.3e}')
        loss_list.append(float(loss))
        acc_list.append(float(accuracy))

        



a = 10

x = torch.linspace(0,2,100)
dy = torch.sin(a*x)
dy_in = dy.unsqueeze(0).unsqueeze(1)
x_in = x.unsqueeze(0).unsqueeze(-1)
out_true = -1/a*(torch.cos(a*x)-1)
-(torch.cos(a*x)-1)
with torch.no_grad():
    out = Onet.cpu()(x_in,dy_in).squeeze()

print(out.shape)

plt.plot(x,out,label = 'Net')
plt.plot(x,out_true,label = 'Analytic')
plt.legend(loc='upper left')
plt.title(f'Indefinite Integral for sin({a}x)')
plt.savefig(f'a_{a}.png')
plt.show()





from torch.func import vmap,jacrev,grad


def batched_PINO_grad(net):
    def aux_net(net):
        def inner_func(*args,**kwargs):
            out = net(*args,**kwargs)
            return out,out
        return inner_func
    
    g = jacrev(aux_net(net),has_aux=True)
    # If y,u are the arg inputs, we only want to iterate over y's first dim (the query dim) not u

    grad_func = vmap(g,(0,None))

    return vmap(grad_func,0)
    





PINO = DeepONet(75,1,branch = MLP(100,75,75,4),trunk = MLP(1,75,75,4))

f = batched_PINO_grad(PINO)
y = torch.rand((5,100,1))
u = torch.rand((5,1,100))

grads,evals = f(y,u)
grads.shape,evals.shape,PINO(y,u).shape


ys,us,Guys = generate_data(10000,0,2,100,100,5,20)
Pino_data = Onet_dataset(ys,us,Guys)
# Onet_data.set_device('cuda')
Pino_DL = DataLoader(Pino_data,batch_size = 1000,shuffle = True)


optimiser = Adam(PINO.parameters())
lr_sch = torch.optim.lr_scheduler.StepLR(optimiser,1000,0.95)


evals[:,0].squeeze()


PINO = PINO.to(device)
optimiser.zero_grad()
x = torch.linspace(0,2,100)
a = 4
dy = torch.sin(a*x)

dy_in = dy.unsqueeze(0).unsqueeze(1)
x_in = x.unsqueeze(0).unsqueeze(-1)
out_true = -1/a*(torch.cos(a*x)-1)
print(dy_in.shape,x_in.shape)

PINO_loss_list= []
PINO_acc_list = []
f = batched_PINO_grad(PINO)
for epoch in range(0,7501):
    for (y,u,Guy) in(Pino_DL):
        (y,u,Guy) = (y.to(device),u.to(device),Guy.to(device))
        # print(y.shape,u.shape,Guy.shape)
        grads,evals = f(y,u)

        #PINN loss
        
        PINN_loss = (grads.squeeze() - u.squeeze()).pow(2).mean()
        
        #Initial Condition Loss
        IC_loss = (evals[:,0].squeeze()).pow(2).mean()

        # print(out.shape,Guy.shape)
        # loss = (out-Guy).pow(2).mean()
        loss = 10*PINN_loss + IC_loss
        loss.backward()
        optimiser.step()
        optimiser.zero_grad()
        lr_sch.step()
    if (epoch % 100) == 0:
        print(f'{float(lr_sch.get_last_lr()[0]):.3E}')
        with torch.no_grad():
            out = PINO.cpu()(x_in,dy_in).squeeze()
            accuracy = (out-out_true).pow(2).mean()
            PINO = PINO.to(device)
        
        print(f'Epoch: {epoch} loss {float(loss):.3e} PINN Loss {float(PINN_loss):.3e} IC Loss {float(IC_loss):.3e} Acc {float(accuracy):.3e}')
        PINO_loss_list.append(float(loss))
        PINO_acc_list.append(float(accuracy))
        



x = torch.linspace(0,2,100)
a = 12
dy = a*torch.sin(a*x)

dy_in = dy.unsqueeze(0).unsqueeze(1)
x_in = x.unsqueeze(0).unsqueeze(-1)
out_true = -1/a*(torch.cos(a*x)-1)
with torch.no_grad():
    out = PINO.cpu()(x_in,dy_in).squeeze()
    out_dd = Onet.cpu()(x_in,dy_in).squeeze()
print(out.shape)
# out = torch.tensor(out).squeeze()
plt.plot(x,out,label = 'Physics Informed')
plt.plot(x,out_dd,label = 'Data Driven')
plt.plot(x,-(torch.cos(a*x)-1),label = 'Analytic')
plt.legend()


x = np.linspace(0,7500,76)

plt.plot(x,PINO_acc_list,label = 'Physics Informed')
plt.plot(x,acc_list,label = 'Data Driven')
plt.yscale('log')
plt.legend()
plt.title()


x = np.linspace(0,7500,76)

plt.plot(x,PINO_loss_list,label = 'Physics Informed')
plt.plot(x,loss_list,label = 'Data Driven')
plt.yscale('log')
plt.legend()


a,b = 0,2

x = torch.linspace(a,b,100)
c = 12
dy = c*torch.sin(c*x)

#Rescale
g = (dy-dy.min())/(dy.max()-dy.min())


dy_in = dy.unsqueeze(0).unsqueeze(1)

g_in = g.unsqueeze(0).unsqueeze(1)
x_in = x.unsqueeze(0).unsqueeze(-1)

# out_true = -1/c*(torch.cos(c*x)-1)
with torch.no_grad():

    G = PINO.cpu()(x_in,g_in).squeeze()
    out = PINO.cpu()(x_in,dy_in).squeeze()
    out_dd = Onet.cpu()(x_in,dy_in).squeeze()
    
    G_dd = Onet.cpu()(x_in,g_in).squeeze()

    dd_rescale = (dy.max()-dy.min())*G_dd + dy.min()*x.squeeze()
    PINO_rescale = (dy.max()-dy.min())*G + dy.min()*x.squeeze()

print(out.shape)
# out = torch.tensor(out).squeeze()
# plt.plot(x,out,label = 'Physics Informed')
# plt.plot(x,PINO_rescale,label = 'Physics Informed Rescaling')
plt.plot(x,out_dd,label = 'Data Driven')
plt.plot(x,dd_rescale,label = 'Data Driven Rescaling')
plt.plot(x,-(torch.cos(c*x)-1),label = 'Analytic')
plt.legend()
plt.show()

plt.plot(x,G,label = 'Physics Informed')
plt.plot(x,g,label = 'Physics Informed')
# plt.plot(x,out_dd,label = 'Data Driven')
# plt.plot(x,,label = 'Analytic')
plt.legend()
plt.show()

