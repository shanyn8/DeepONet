


# Environment & imports
import math, time, numpy as np
import torch, torch.nn as nn
import matplotlib.pyplot as plt

device = ("cuda" if torch.cuda.is_available()
          else "mps" if getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available()
          else "cpu")
print('Using device:', device)
torch.set_default_dtype(torch.float32)
_ = torch.manual_seed(1234)






# === Config ===
m_sensors   = 64      # number of Chebyshev sensors for branch input f
width       = 128     # feature width (branch/trunk)
depth       = 3       # hidden layers in branch/trunk
B_batch     = 16      # number of functions per batch (operator learning)
N_col       = 256     # collocation points per function
N_bc        = 2       # number of BC points per boundary per function
steps       = 10    # training steps
lr          = 1e-3    # learning rate

# Distribution for k (avoid resonance k≈1 and sin(k)≈0 for the test analytic comparison)
k = 18.0

# Random forcing generator: Chebyshev series with decaying coefficients
Nf_terms = 12         # number of Chebyshev terms for random f
decay    = 0.8        # geometric decay factor for coeff magnitudes (0<decay<1)

# Loss weights
w_pde, w_bc = 1.0, 1.0









# Change range to [0, 1] with u(0)=u(1)=0
def cheb_sensors(m):
    dtype = torch.float32 if device == "mps" else torch.float64
    j = torch.arange(m, dtype=dtype, device=device)
    # j = torch.arange(m, dtype=torch.float64, device=device)
    xi = torch.cos((2*j+1)*math.pi/(2*m))
    return xi.to(torch.float32)


# Unit test
def test_cheb_sensors():
    m = 5
    xi = cheb_sensors(m)
    # Check type
    assert isinstance(xi, torch.Tensor), "Output should be a torch.Tensor"
    # Check shape
    assert xi.shape == (m,), f"Expected shape ({m},), got {xi.shape}"
    # Check dtype
    assert xi.dtype == torch.float32, f"Expected dtype torch.float32, got {xi.dtype}"
    # Check values are within [-1, 1]
    assert torch.all(xi <= 1) and torch.all(xi >= -1), "Values should be in [-1, 1]"
    # Check monotonicity (Chebyshev points should decrease from 1 to -1)
    assert torch.all(xi[:-1] > xi[1:]), "Chebyshev points should be strictly decreasing"

test_cheb_sensors()
    
# TODO - check replace with SCIPY BARYCENTRIC INTERPOLATION
def cheb_basis(x, n_terms):
    """Return [T0(x),...,T_{n-1}(x)] as a matrix of shape (len(x), n_terms).
    x: (N,1) or (N,) torch tensor in [-1,1]
    """
    x = x.view(-1,1)
    N = x.shape[0]
    T = torch.zeros(N, n_terms, device=device)
    if n_terms>=1: T[:,0] = 1.0
    if n_terms>=2: T[:,1] = x[:,0]
    for n in range(2, n_terms):
        T[:,n] = 2*x[:,0]*T[:,n-1] - T[:,n-2]
    return T



x_k = cheb_sensors(m_sensors).to(device)        # (m,) Chebyshev sensors
x_col  = -1 + 2*torch.rand(N_col, 1, device=device)



import unittest

import torch
import numpy as np

def chebyshev_diff_matrix(N: int):
    """
    Compute Chebyshev differentiation matrix and nodes using Trefethen's algorithm.
    Args:
        N (int): Number of intervals (matrix size will be (N+1)x(N+1))
    Returns:
        x (torch.Tensor): Chebyshev nodes
        D (torch.Tensor): Differentiation matrix
    """
    if N < 1:
        raise ValueError("N must be >= 1")

    # Chebyshev nodes
    # x = torch.cos(torch.pi * torch.arange(N + 1) / N)
    x = cheb_sensors(N)
    # Coefficients for scaling
    c = torch.ones(N + 1)
    c[0], c[-1] = 2, 2
    c = c * ((-1) ** torch.arange(N + 1))

    # Compute differentiation matrix
    X = x.repeat(N + 1, 1)
    dX = X - X.T
    D = torch.outer(c, 1 / c) / (dX + torch.eye(N + 1))
    D = D - torch.diag(torch.sum(D, dim=1))

    # Fix diagonal entries
    for i in range(N + 1):
        if i == 0:
            D[i, i] = (2 * N ** 2 + 1) / 6
        elif i == N:
            D[i, i] = -(2 * N ** 2 + 1) / 6
        else:
            D[i, i] = -x[i] / (2 * (1 - x[i] ** 2))

    return x, D


# # -----------------------------
# # 1. Chebyshev Nodes and Diff Matrix
# # -----------------------------
# def chebyshev_nodes(N):
#     k = torch.arange(N + 1, dtype=torch.float32)
#     x = torch.cos(torch.pi * k / N)
#     return x.unsqueeze(1)

# def chebyshev_diff_matrix(N):
#     x = torch.cos(torch.pi * torch.arange(N + 1) / N)
#     c = torch.ones(N + 1)
#     c[0], c[-1] = 2, 2
#     c = c * ((-1) ** torch.arange(N + 1))
#     X = x.repeat(N + 1, 1)
#     dX = X - X.T
#     D = torch.outer(c, 1 / c) / (dX + torch.eye(N + 1))
#     D = D - torch.diag(torch.sum(D, dim=1))
#     for i in range(N + 1):
#         if i == 0:
#             D[i, i] = (2 * N ** 2 + 1) / 6
#         elif i == N:
#             D[i, i] = -(2 * N ** 2 + 1) / 6
#         else:
#             D[i, i] = -x[i] / (2 * (1 - x[i] ** 2))
#     return x.unsqueeze(1), D
    
    
class TestChebyshevDiffMatrix(unittest.TestCase):
    def test_shape(self):
        for N in [1, 5, 10]:
            x, D = chebyshev_diff_matrix(N)
            self.assertEqual(D.shape, (N + 1, N + 1))

    def test_row_sum_zero(self):
        for N in [5, 10, 20]:
            _, D = chebyshev_diff_matrix(N)
            row_sums = torch.sum(D, dim=1)
            self.assertTrue(torch.allclose(row_sums, torch.zeros_like(row_sums), atol=1e-8))

    def test_derivative_accuracy(self):
        for N in [10, 20]:
            x, D = chebyshev_diff_matrix(N)
            u = x ** 2
            du_numeric = D @ u
            du_exact = 2 * x
            error = torch.norm(du_numeric - du_exact, p=float('inf'))
            self.assertTrue(error < 1e-6)

    def test_invalid_N(self):
        with self.assertRaises(ValueError):
            chebyshev_diff_matrix(0)

# TestChebyshevDiffMatrix.test_invalid_N(self)

if __name__ == "__main__":
    unittest.main()


# import unittest
# from test_chebyshev import TestChebyshevDiffMatrix

suite = unittest.TestSuite()
# suite.addTest(TestChebyshevDiffMatrix('test_shape'))
suite.addTest(TestChebyshevDiffMatrix('test_row_sum_zero'))
# suite.addTest(TestChebyshevDiffMatrix('test_invalid_N'))
# suite.addTest(TestChebyshevDiffMatrix('test_derivative_accuracy'))

unittest.TextTestRunner().run(suite)





import torch
import matplotlib.pyplot as plt

# Function to interpolate
def f(x):
    # k = 0.8
    # A = 1.0/((k**2)-1.0)
    # B = -A*math.sin(1.0)/math.sin(k)
    # return A*torch.sin(x) + B*torch.sin(k*x)
    return 1 / (1 + 25 * x**2)

# Generate Chebyshev nodes of the first kind
def chebyshev_nodes(N):
    k = torch.arange(N + 1)
    x_k = torch.cos((2 * k + 1) * torch.pi / (2 * (N + 1)))
    return x_k

# Compute barycentric weights (O(N^2) version)
def barycentric_weights(x_k):
    N = len(x_k) - 1
    w = torch.ones(N + 1)
    for j in range(N + 1):
        for k in range(N + 1):
            if j != k:
                w[j] /= (x_k[j] - x_k[k])
    return w

# Barycentric interpolation
def barycentric_interpolate(x_eval, x_k, f_k, w):
    P = torch.zeros_like(x_eval)
    for i, x in enumerate(x_eval):
        diff = x - x_k
        if torch.any(diff == 0):
            P[i] = f_k[torch.where(diff == 0)[0][0]]
        else:
            terms = w / diff
            P[i] = torch.sum(terms * f_k) / torch.sum(terms)
    return P


def barycentric_interpolate_eval(x_k, f_k, x_eval):
    w = barycentric_weights(x_k)
    P_eval = barycentric_interpolate(x_eval, x_k, f_k, w)
    return P_eval
    
# Degree N
N = 20
x_k = chebyshev_nodes(N)
f_k = f(x_k)
# w = barycentric_weights(x_k)

# Evaluation points
x_eval = torch.linspace(-1, 1, 500)
f_eval = f(x_eval)
P_eval = barycentric_interpolate_eval(x_k, f_k, x_eval)


# Plot
plt.figure(figsize=(10, 6))
plt.plot(x_eval.numpy(), f_eval.numpy(), label='Original Function f(x)', linewidth=2)
plt.plot(x_eval.numpy(), P_eval.numpy(), label='Interpolated Polynomial', linestyle='--')
plt.scatter(x_k.numpy(), f_k.numpy(), color='red', label='Chebyshev Nodes')
plt.title('Barycentric Interpolation using Chebyshev Nodes')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()



x_k





# class BranchNet(nn.Module):
#     def __init__(self, m, width=128, depth=3, act=nn.Tanh):
#         super().__init__()
#         layers = [nn.Linear(m, width), act()]
#         for _ in range(depth-1):
#             layers += [nn.Linear(width, width), act()]
#         self.net = nn.Sequential(*layers)
#     def forward(self, f_samples):
#         return self.net(f_samples)

# class TrunkNet(nn.Module):
#     def __init__(self, width=128, depth=3, act=nn.Tanh):
#         super().__init__()
#         layers = [nn.Linear(1, width), act()]
#         for _ in range(depth-1):
#             layers += [nn.Linear(width, width), act()]
#         self.net = nn.Sequential(*layers)
#     def forward(self, x):
#         return self.net(x)

# class DeepONet(nn.Module):
#     def __init__(self, m, width=128, depth=3):
#         super().__init__()
#         self.branch = BranchNet(m, width, depth)
#         self.trunk  = TrunkNet(width, depth)
#         self.bias   = nn.Parameter(torch.zeros(1))
#     def forward(self, f_batch, x_batch):
#         # f_batch: (B, m), x_batch: (B*N, 1)
#         B, m = f_batch.shape
#         N     = x_batch.shape[0] // B
#         bfeat = self.branch(f_batch)                 # (B, W)
#         tfeat = self.trunk(x_batch)                  # (B*N, W)
#         btile = bfeat.repeat_interleave(N, dim=0)    # (B*N, W)
#         u = (btile * tfeat).sum(dim=1, keepdim=True) + self.bias  # inner product
#         return u


import torch
import torch.nn as nn

# Branch network: encodes forcing function samples
class BranchNet(nn.Module):
    def __init__(self, m, width=128, depth=3, act=nn.Tanh):
        super().__init__()
        layers = [nn.Linear(m, width), act()]
        for _ in range(depth - 1):
            layers += [nn.Linear(width, width), act()]
        self.net = nn.Sequential(*layers)

    def forward(self, f_samples):
        # f_samples: shape (m,) or (1, m)
        return self.net(f_samples)

# Trunk network: encodes spatial coordinate x
class TrunkNet(nn.Module):
    def __init__(self, width=128, depth=3, act=nn.Tanh):
        super().__init__()
        layers = [nn.Linear(1, width), act()]
        for _ in range(depth - 1):
            layers += [nn.Linear(width, width), act()]
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        # x: shape (N, 1)
        return self.net(x)

# DeepONet: combines branch and trunk features
class DeepONet(nn.Module):
    def __init__(self, m, width=128, depth=3):
        super().__init__()
        self.branch = BranchNet(m, width, depth)
        self.trunk = TrunkNet(width, depth)
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, f_samples, x_points):
        # f_samples: (1, m), x_points: (N, 1)
        bfeat = self.branch(f_samples)        # (1, W)
        tfeat = self.trunk(x_points)          # (N, W)
        # Broadcast branch features to match trunk features
        u = (bfeat * tfeat).sum(dim=1, keepdim=True) + self.bias
        return u  # shape (N, 1)

model = DeepONet(m_sensors, width=width, depth=depth).to(device)
print(f"Num of model params: {sum(p.numel() for p in model.parameters())}")







x_k = cheb_sensors(m_sensors).to(device)        # (m,) Chebyshev sensors
print(f"Create {xi.shape} sensonrs xi")






mse = nn.MSELoss()
opt = torch.optim.Adam(model.parameters(), lr=lr)


def train_step():
    # print("train_step")

    # x in collacation points (eval) - should be const or randomly change in each train step?
    x_col  = -1 + 2*torch.rand(N_col, 1, device=device)


    # f_sens - f on chebyshev nodes (m points)
    # f_sens = ???
    coeffs = sample_random_f(B)
    f_sens = cheb_eval(xi.view(-1,1), coeffs).squeeze(-1)   # (B, m)   



    
    # convert f to be on the collacation points (barycentric interpolatation on chebyshev nodes)
    # f_vals - result of evaluation of the interpolant on the collacatoin points x_col
    
    # Predict u on x_col - collocation, f_sen - f at chebyshev sensors
    u_pred = model(f_sens, x_col)
    
    # u_pred should be on chebyshev nodes (m values)
    # u_pred -> should do barycentric interpolation on cheb points (on the N-2 intermediate points + boundary points) 
    # u_pred_eval - result of evaluation of the interpolant on the collacatoin points x_col
    
    # Now

    D = chebyshev_diff_matrix(N_col)
    du = D @ u_pred_eval
    d2u = D @ du
    # All should be on collacation points
    # and enfoce u(-1)=u(1)=0  and then eval on collacation 
    # same for f_vals ?
    residual = d2u + k_val**2 * u_pred_eval - f_vals    
    loss = mse(r, torch.zeros_like(r))

    opt.zero_grad()
    loss.backward()
    opt.step()
    # print("done")
    return loss.item()






hist = []
t0 = time.time()
for it in range(1, steps+1):
    # print(f"iteration {it}")
    L, Lp, Lb = train_step()
    hist.append((L,Lp,Lb))
    if it % 200 == 0:
        dt = time.time()-t0
        print(f"iter {it:5d} | loss {L:.3e}  (pde {Lp:.3e}, bc {Lb:.3e}) | {dt:.1f}s")

plt.figure(figsize=(5,3))
plt.semilogy([h[0] for h in hist], label='total')
plt.semilogy([h[1] for h in hist], label='pde')
plt.semilogy([h[2] for h in hist], label='bc')
plt.legend(); plt.title('Training loss'); plt.tight_layout(); plt.show()






def analytic_u_sin(x, k):
    A = 1.0/((k**2)-1.0)
    B = -A*math.sin(1.0)/math.sin(k)
    return A*torch.sin(x) + B*torch.sin(k*x)

def test_on_sin(k_test=8.0):
    f_sens = torch.sin(xi).unsqueeze(0)           # (1,m)
    x = torch.linspace(-1, 1, 1000).unsqueeze(1)
    f_sens = f_sens.to(device); x = x.to(device)
    with torch.no_grad():
        u_hat = model(f_sens, x)
    u_ex = analytic_u_sin(x, k_test).to(device)
    rel_l2 = torch.linalg.norm(u_hat - u_ex)/torch.linalg.norm(u_ex)
    print(f"Relative L2 error vs analytic (f=sin, k={k_test}): {rel_l2.item():.3e}")
    plt.figure(figsize=(6,3))
    plt.plot(x.cpu(), u_ex.cpu(), 'k-', lw=2, label='analytic')
    plt.plot(x.cpu(), u_hat.cpu(), '--', lw=2, label='DeepONet')
    plt.legend(); plt.title('Test: f(x)=sin x'); plt.tight_layout(); plt.show()

test_on_sin(8.0)






def eval_random_case():
    coeffs = sample_random_f(1)
    x = torch.linspace(-1,1,1000).unsqueeze(1).to(device)
    f_sens = cheb_eval(xi.view(-1,1), coeffs).squeeze(-1)  # (1,m)
    with torch.no_grad():
        u_hat = model(f_sens, x)
        f_x   = cheb_eval(x, coeffs).squeeze(0)            # (1000,1)
    plt.figure(figsize=(10,3))
    plt.subplot(1,2,1); plt.plot(x.cpu(), f_x.cpu()); plt.title('Random f(x)');
    plt.subplot(1,2,2); plt.plot(x.cpu(), u_hat.cpu()); plt.title('u(x) predicted');
    plt.tight_layout(); plt.show()

eval_random_case()






torch.save({'model_state': model.state_dict(),
            'config': {'m_sensors': m_sensors, 'width': width, 'depth': depth}},
           'pi_deeponet_helmholtz.pt')
print('Saved to pi_deeponet_helmholtz.pt')

